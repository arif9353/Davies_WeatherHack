{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas statsmodels matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('./data/history_data.csv')\n",
    "\n",
    "# Convert 'DateTime' to datetime format and set as index\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Define pollutants of interest\n",
    "pollutants = ['PM10', 'PM2.5', 'CO', 'SO2', 'NO2', 'O3']\n",
    "\n",
    "# Function to forecast next 8 hours using ARIMA\n",
    "def forecast_pollutant(data, pollutant, current_value, hours=8):\n",
    "    model = ARIMA(data[pollutant], order=(5,1,0)) # ARIMA with p=5, d=1, q=0 (adjust based on the pollutant)\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Forecast the next 'hours' values\n",
    "    forecast = model_fit.forecast(steps=hours)\n",
    "    \n",
    "    # Replace the first forecast value with the current input value (to align)\n",
    "    forecast[0] = current_value\n",
    "    return forecast\n",
    "\n",
    "# Given current pollutant values\n",
    "current_values = {\n",
    "    \"PM10\": 39.37,\n",
    "    \"PM2.5\": 33.67,\n",
    "    \"SO2\": 2.38,\n",
    "    \"NO2\": 6.25,\n",
    "    \"CO\": 13.35,\n",
    "    \"O3\": 0.10\n",
    "}\n",
    "\n",
    "# Forecast for the next 8 hours for each pollutant\n",
    "forecasts = {}\n",
    "for pollutant in pollutants:\n",
    "    forecast = forecast_pollutant(data, pollutant, current_values[pollutant])\n",
    "    forecasts[pollutant] = forecast\n",
    "\n",
    "# Display forecast results\n",
    "forecasts_df = pd.DataFrame(forecasts)\n",
    "# print(\"\\n\\nForecast\\n\")\n",
    "print(forecasts_df)\n",
    "\n",
    "# Plotting forecast for each pollutant\n",
    "forecasts_df.plot(figsize=(10,6), marker='o')\n",
    "plt.title('Pollutant Forecasts for Next 8 Hours')\n",
    "plt.ylabel('Pollutant Levels')\n",
    "plt.xlabel('Hours Ahead')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Importing necessary libraries\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Cell 2: Load dataset\n",
    "data = pd.read_csv('./data/history_data.csv')  # Update the path to your local dataset\n",
    "\n",
    "# Convert 'DateTime' to datetime format and set frequency to hourly\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data.set_index('DateTime', inplace=True)\n",
    "data = data.asfreq('h')  # Ensure continuous hourly frequency\n",
    "\n",
    "# Fill any missing data after setting frequency\n",
    "data = data.ffill()\n",
    "\n",
    "# Cell 3: Define pollutants of interest\n",
    "pollutants = ['PM10', 'PM2.5', 'CO', 'SO2', 'NO2', 'O3']\n",
    "\n",
    "# Cell 4: Function to apply ARIMA model and forecast next 8 hours\n",
    "def forecast_pollutant(data, pollutant, current_value, hours=8):\n",
    "    ts = data[pollutant]\n",
    "    \n",
    "    # Ensure there's enough data to fit the model\n",
    "    if len(ts) > 10:\n",
    "        try:\n",
    "            # Fit ARIMA model with (5,1,0)\n",
    "            model = ARIMA(ts, order=(5,1,0))\n",
    "            model_fit = model.fit()\n",
    "        except:\n",
    "            # If error occurs, adjust ARIMA parameters and retry\n",
    "            model = ARIMA(ts, order=(1,1,0))\n",
    "            model_fit = model.fit()\n",
    "        \n",
    "        # Forecast the next 'hours' values\n",
    "        forecast = model_fit.forecast(steps=hours)\n",
    "        \n",
    "        # Replace the first forecast value with the current input value (to align)\n",
    "        forecast.iloc[0] = current_value\n",
    "        return forecast\n",
    "    else:\n",
    "        print(f\"Not enough data to fit model for {pollutant}\")\n",
    "        return np.array([current_value]*hours)\n",
    "\n",
    "# Cell 5: Given current pollutant values\n",
    "current_values = {\n",
    "    \"PM10\": 89.37,\n",
    "    \"PM2.5\": 60.67,\n",
    "    \"SO2\": 2.38,\n",
    "    \"NO2\": 6.25,\n",
    "    \"CO\": 13.35,\n",
    "    \"O3\": 0.10\n",
    "}\n",
    "\n",
    "# Cell 6: Forecast for the next 8 hours for each pollutant\n",
    "forecasts = {}\n",
    "for pollutant in pollutants:\n",
    "    forecast = forecast_pollutant(data, pollutant, current_values[pollutant])\n",
    "    forecasts[pollutant] = forecast\n",
    "\n",
    "# Display forecast results\n",
    "forecasts_df = pd.DataFrame(forecasts)\n",
    "print(forecasts_df)\n",
    "\n",
    "# Cell 7: Plotting forecast for each pollutant\n",
    "forecasts_df.plot(figsize=(10,6), marker='o')\n",
    "plt.title('Pollutant Forecasts for Next 8 Hours')\n",
    "plt.ylabel('Pollutant Levels')\n",
    "plt.xlabel('Hours Ahead')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib  # For saving the model and scaler\n",
    "\n",
    "# Cell 2: Load dataset\n",
    "data = pd.read_csv('./data/history_data.csv')  # Update the path to your local dataset\n",
    "\n",
    "# Convert 'DateTime' to datetime format and set frequency to hourly\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data.set_index('DateTime', inplace=True)\n",
    "data = data.asfreq('h')  # Ensure continuous hourly frequency\n",
    "\n",
    "# Fill any missing data after setting frequency\n",
    "data = data.ffill()\n",
    "\n",
    "# Cell 3: Define pollutants of interest\n",
    "pollutants = ['PM10', 'PM2.5', 'CO', 'SO2', 'NO2', 'O3']\n",
    "\n",
    "# Cell 4: Function to apply ARIMA model and forecast next 8 hours\n",
    "# Fine-tuned ARIMA with short memory\n",
    "def forecast_pollutant(current_value, ts, hours=8):\n",
    "    # Model ARIMA with short memory (p=1, d=1, q=1)\n",
    "    model = ARIMA(ts, order=(1,1,1))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Save the model to a file\n",
    "    model_filename = f\"arima_model_{pollutant}.pkl\"\n",
    "    joblib.dump(model_fit, model_filename)  # Saving model\n",
    "\n",
    "    # Forecast the next 'hours' values based on the current trend\n",
    "    forecast = model_fit.forecast(steps=hours)\n",
    "    \n",
    "    # Adjust the forecast to start from the current value\n",
    "    forecast = forecast + (current_value - forecast[0])\n",
    "    \n",
    "    return forecast\n",
    "\n",
    "# Cell 5: Given current pollutant values\n",
    "current_values = {\n",
    "    \"PM10\": 55.37,  # Updated value for PM10\n",
    "    \"PM2.5\": 33.67,\n",
    "    \"SO2\": 2.38,\n",
    "    \"NO2\": 6.25,\n",
    "    \"CO\": 13.35,\n",
    "    \"O3\": 0.10\n",
    "}\n",
    "\n",
    "# Cell 6: Forecast for the next 8 hours for each pollutant\n",
    "forecasts = {}\n",
    "for pollutant in pollutants:\n",
    "    # Get the historical time series for the pollutant\n",
    "    ts = data[pollutant]\n",
    "\n",
    "    # Use the current value provided as the base for forecasting\n",
    "    forecast = forecast_pollutant(current_values[pollutant], ts)\n",
    "    forecasts[pollutant] = forecast\n",
    "\n",
    "# Display forecast results\n",
    "forecasts_df = pd.DataFrame(forecasts)\n",
    "print(forecasts_df)\n",
    "\n",
    "# Cell 7: Plotting forecast for each pollutant\n",
    "forecasts_df.plot(figsize=(10,6), marker='o')\n",
    "plt.title('Pollutant Forecasts for Next 8 Hours Based on Current Values')\n",
    "plt.ylabel('Pollutant Levels')\n",
    "plt.xlabel('Hours Ahead')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Loading the saved ARIMA model and using it for forecasting\n",
    "import joblib\n",
    "\n",
    "# Cell 9: Load the ARIMA model from a file\n",
    "pollutant = 'PM10'  # Change this to load the model for any other pollutant\n",
    "model_filename = f\"arima_model_{pollutant}.pkl\"\n",
    "loaded_model = joblib.load(model_filename)\n",
    "\n",
    "# Cell 10: Forecast using the loaded model\n",
    "# You can change the `current_value` if needed\n",
    "current_value = 52\n",
    "\n",
    "# Perform forecast for the next 8 hours\n",
    "forecast = loaded_model.forecast(steps=8)\n",
    "\n",
    "# Adjust the forecast to start from the current value\n",
    "forecast = forecast + (current_value - forecast[0])\n",
    "\n",
    "print(f\"Forecast for {pollutant}:\")\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Importing necessary libraries\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Cell 2: Load dataset\n",
    "data = pd.read_csv('./data/history_data.csv')  # Update the path to your local dataset\n",
    "\n",
    "# Convert 'DateTime' to datetime format and set frequency to hourly\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data.set_index('DateTime', inplace=True)\n",
    "data = data.asfreq('h')  # Ensure continuous hourly frequency\n",
    "\n",
    "# Fill any missing data after setting frequency\n",
    "data = data.ffill()\n",
    "\n",
    "# Cell 3: Given current pollutant values (these are the inputs for forecasting)\n",
    "current_values = {\n",
    "    \"PM10\": 55.37,  # Updated value for PM10\n",
    "    \"PM2.5\": 33.67,\n",
    "    \"SO2\": 2.38,\n",
    "    \"NO2\": 6.25,\n",
    "    \"CO\": 13.35,\n",
    "    \"O3\": 0.10\n",
    "}\n",
    "\n",
    "# Cell 4: Function to apply ARIMA model and forecast next 8 hours\n",
    "# Tuned ARIMA model with more sensitivity to past fluctuations\n",
    "def forecast_pollutant(current_value, ts, hours=8):\n",
    "    # Model ARIMA with larger p and q values to capture more fluctuations\n",
    "    model = ARIMA(ts, order=(3,1,3))  # Increased p and q to add variability\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Forecast the next 'hours' values based on the current trend\n",
    "    forecast = model_fit.forecast(steps=hours)\n",
    "\n",
    "    # Adjust the forecast to start from the current value\n",
    "    forecast = forecast + (current_value - forecast[0])\n",
    "    \n",
    "    return forecast\n",
    "\n",
    "# Cell 5: Forecast for the next 8 hours for each pollutant\n",
    "pollutant = 'PM10'  # Let's focus on PM10 first\n",
    "ts = data[pollutant]\n",
    "\n",
    "# Use the current value provided as the base for forecasting\n",
    "forecast = forecast_pollutant(current_values[pollutant], ts)\n",
    "\n",
    "# Display forecast results\n",
    "forecast.index = pd.date_range(start=pd.to_datetime('2024-09-03 20:00:00'), periods=len(forecast), freq='H')\n",
    "print(f\"Forecast for {pollutant}:\")\n",
    "print(forecast)\n",
    "\n",
    "# Cell 6: Plotting forecast for the pollutant\n",
    "# forecast.plot(figsize=(10,6), marker='o')\n",
    "# plt.title(f'Forecast for {pollutant} (Next 8 Hours)')\n",
    "# plt.ylabel(f'{pollutant} Level')\n",
    "# plt.xlabel('Time')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Fetch data from API\n",
    "data = requests.get(\"https://blogcontent.site/projects/aqi24.php\").json()\n",
    "\n",
    "# Load API data into pandas DataFrame\n",
    "df_api = pd.DataFrame(data)\n",
    "\n",
    "# Load historical data\n",
    "file_path = './data/history_data.csv'  # Change this to the correct file path\n",
    "df_history = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'time' in API data to datetime and set it as index\n",
    "df_api['time'] = pd.to_datetime(df_api['time'])\n",
    "df_api.set_index('time', inplace=True)\n",
    "\n",
    "# Combine API AQI data and historical AQI data for training\n",
    "aqi_history = df_history[['AQI']].values\n",
    "aqi_api = df_api[['aqi_val']].values\n",
    "\n",
    "# Combine both AQI data\n",
    "aqi_combined = np.concatenate((aqi_history, aqi_api), axis=0)\n",
    "\n",
    "# Scale the AQI data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_aqi = scaler.fit_transform(aqi_combined)\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(data, time_steps=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set time steps as 24 (for 24-hour sequences)\n",
    "time_steps = 24\n",
    "X, y = create_sequences(scaled_aqi, time_steps)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding LSTM layer with 50 units\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_steps, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding another LSTM layer\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding Dense output layer for predicting AQI\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Forecasting the next 8 hours\n",
    "n_forecast = 8\n",
    "X_input = scaled_aqi[-time_steps:].reshape(1, time_steps, 1)\n",
    "forecast = []\n",
    "\n",
    "for i in range(n_forecast):\n",
    "    next_pred = model.predict(X_input)\n",
    "    forecast.append(next_pred[0, 0])\n",
    "    \n",
    "    # Update the input sequence for the next prediction\n",
    "    X_input = np.append(X_input[:, 1:, :], next_pred.reshape(1, 1, 1), axis=1)\n",
    "\n",
    "# Inverse transform the forecasted values to original scale\n",
    "forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n",
    "\n",
    "# Print the forecasted AQI values for the next 8 hours\n",
    "print(\"Forecasted AQI values for the next 8 hours:\", forecast)\n",
    "\n",
    "# Plot the forecasted AQI values\n",
    "forecast_index = pd.date_range(start=df_api.index[-1], periods=n_forecast+1, freq='H')[1:]\n",
    "plt.plot(forecast_index, forecast, label='Forecasted AQI', color='red')\n",
    "plt.title(\"AQI Forecast for Next 8 Hours\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"AQI\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Fetch the most recent API data\n",
    "data = requests.get(\"https://blogcontent.site/projects/aqi24.php\").json()\n",
    "# data = data[::-1]  # Reverse the data to maintain correct chronological order\n",
    "df_recent = pd.DataFrame(data)\n",
    "df_recent['time'] = pd.to_datetime(df_recent['time'])\n",
    "df_recent.set_index('time', inplace=True)\n",
    "\n",
    "# Load historical data\n",
    "file_path = './data/history_data.csv'  # Change this to the correct file path\n",
    "df_history = pd.read_csv(file_path)\n",
    "\n",
    "# Combine recent and historical data\n",
    "df_history['DateTime'] = pd.to_datetime(df_history['DateTime'])\n",
    "df_history.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Only use the AQI column\n",
    "df_combined = pd.concat([df_history[['AQI']], df_recent[['aqi_val']].rename(columns={'aqi_val': 'AQI'})])\n",
    "\n",
    "# Convert AQI column to numeric\n",
    "df_combined['AQI'] = pd.to_numeric(df_combined['AQI'], errors='coerce')\n",
    "\n",
    "# Apply differencing to make the series stationary\n",
    "df_combined['AQI_diff'] = df_combined['AQI'].diff().dropna()\n",
    "\n",
    "# Scale the differenced AQI data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_aqi = scaler.fit_transform(df_combined[['AQI_diff']].dropna())\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(data, time_steps=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set time steps as 24 (for 24-hour sequences)\n",
    "time_steps = 24\n",
    "X, y = create_sequences(scaled_aqi, time_steps)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding LSTM layer with 64 units\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(time_steps, 1)))\n",
    "model.add(Dropout(0.1))  # Reduced dropout\n",
    "\n",
    "# Adding another LSTM layer\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.1))  # Reduced dropout\n",
    "\n",
    "# Adding Dense output layer for predicting AQI\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with more epochs\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Forecasting the next 8 hours\n",
    "n_forecast = 8\n",
    "X_input = scaled_aqi[-time_steps:].reshape(1, time_steps, 1)\n",
    "forecast_diff = []\n",
    "\n",
    "for i in range(n_forecast):\n",
    "    next_pred = model.predict(X_input)\n",
    "    forecast_diff.append(next_pred[0, 0])\n",
    "    \n",
    "    # Update the input sequence for the next prediction\n",
    "    X_input = np.append(X_input[:, 1:, :], next_pred.reshape(1, 1, 1), axis=1)\n",
    "\n",
    "# Inverse transform the forecasted values to original scale\n",
    "forecast_diff = np.array(forecast_diff).reshape(-1, 1)\n",
    "forecast_diff = scaler.inverse_transform(forecast_diff)\n",
    "\n",
    "# Revert differencing to get actual AQI forecast values\n",
    "last_known_aqi = df_combined['AQI'].iloc[-1]\n",
    "forecast = last_known_aqi + np.cumsum(forecast_diff)\n",
    "\n",
    "# Print the forecasted AQI values for the next 8 hours\n",
    "print(\"Forecasted AQI values for the next 8 hours:\", forecast)\n",
    "\n",
    "# Plot the forecasted AQI values\n",
    "forecast_index = pd.date_range(start=df_recent.index[-1], periods=n_forecast+1, freq='H')[1:]\n",
    "plt.plot(forecast_index, forecast, label='Forecasted AQI', color='red')\n",
    "plt.title(\"AQI Forecast for Next 8 Hours\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"AQI\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "# Fetch the most recent API data\n",
    "data = requests.get(\"https://blogcontent.site/projects/aqi24.php\").json()\n",
    "data = data[::-1]  # Reverse the data to maintain correct chronological order\n",
    "df_recent = pd.DataFrame(data)\n",
    "df_recent['time'] = pd.to_datetime(df_recent['time'])\n",
    "df_recent.set_index('time', inplace=True)\n",
    "\n",
    "# Load historical data\n",
    "file_path = './data/history_data.csv'  # Change this to the correct file path\n",
    "df_history = pd.read_csv(file_path)\n",
    "\n",
    "# Combine recent and historical data\n",
    "df_history['DateTime'] = pd.to_datetime(df_history['DateTime'])\n",
    "df_history.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Only use the AQI column\n",
    "df_combined = pd.concat([df_history[['AQI']], df_recent[['aqi_val']].rename(columns={'aqi_val': 'AQI'})])\n",
    "\n",
    "# Convert AQI column to numeric\n",
    "df_combined['AQI'] = pd.to_numeric(df_combined['AQI'], errors='coerce')\n",
    "\n",
    "# Apply differencing to make the series stationary\n",
    "df_combined['AQI_diff'] = df_combined['AQI'].diff().dropna()\n",
    "\n",
    "# Create a trend feature (1 for increase, 0 for decrease)\n",
    "df_combined['trend'] = np.where(df_combined['AQI_diff'] > 0, 1, 0)\n",
    "\n",
    "# Scale the differenced AQI data and trend using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_aqi = scaler.fit_transform(df_combined[['AQI_diff', 'trend']].dropna())\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(data, time_steps=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps][0])  # Predicting the AQI_diff\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set time steps as 24 (for 24-hour sequences)\n",
    "time_steps = 24\n",
    "X, y = create_sequences(scaled_aqi, time_steps)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding Bidirectional LSTM layer with 64 units\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(time_steps, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Adding another Bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Adding Dense output layer for predicting AQI_diff\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with more epochs\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Forecasting the next 8 hours\n",
    "n_forecast = 8\n",
    "X_input = scaled_aqi[-time_steps:].reshape(1, time_steps, 2)\n",
    "forecast_diff = []\n",
    "\n",
    "for i in range(n_forecast):\n",
    "    next_pred = model.predict(X_input)\n",
    "    forecast_diff.append(next_pred[0, 0])\n",
    "    \n",
    "    # Update the input sequence for the next prediction\n",
    "    new_trend = 1 if next_pred > 0 else 0  # Update trend based on prediction\n",
    "    next_input = np.array([[next_pred[0, 0], new_trend]]).reshape(1, 1, 2)\n",
    "    X_input = np.append(X_input[:, 1:, :], next_input, axis=1)\n",
    "\n",
    "# Inverse transform the forecasted values to original scale\n",
    "forecast_diff = np.array(forecast_diff).reshape(-1, 1)\n",
    "forecast_diff = scaler.inverse_transform(np.hstack([forecast_diff, np.zeros_like(forecast_diff)]))[:, 0]\n",
    "\n",
    "# Revert differencing to get actual AQI forecast values\n",
    "last_known_aqi = df_combined['AQI'].iloc[-1]\n",
    "forecast = last_known_aqi + np.cumsum(forecast_diff)\n",
    "\n",
    "# Print the forecasted AQI values for the next 8 hours\n",
    "print(\"Forecasted AQI values for the next 8 hours:\", forecast)\n",
    "\n",
    "# Plot the forecasted AQI values\n",
    "forecast_index = pd.date_range(start=df_recent.index[-1], periods=n_forecast+1, freq='H')[1:]\n",
    "plt.plot(forecast_index, forecast, label='Forecasted AQI', color='red')\n",
    "plt.title(\"AQI Forecast for Next 8 Hours\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"AQI\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - loss: 1.3933e-04 - val_loss: 1.8183e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 3.2515e-05 - val_loss: 1.8191e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 3.0010e-05 - val_loss: 1.8547e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 2.9592e-05 - val_loss: 1.8671e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.8733e-05 - val_loss: 1.8234e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.8569e-05 - val_loss: 1.8289e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.8479e-05 - val_loss: 1.8079e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 3.0405e-05 - val_loss: 1.8141e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.9653e-05 - val_loss: 1.8258e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7944e-05 - val_loss: 1.8147e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7403e-05 - val_loss: 1.8307e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7278e-05 - val_loss: 1.8086e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 2.6905e-05 - val_loss: 1.8149e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.6425e-05 - val_loss: 1.8130e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.4908e-05 - val_loss: 1.8154e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7616e-05 - val_loss: 1.8575e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7170e-05 - val_loss: 1.8146e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.6388e-05 - val_loss: 1.8317e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7782e-05 - val_loss: 1.8281e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.6644e-05 - val_loss: 1.8196e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7647e-05 - val_loss: 1.8326e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7603e-05 - val_loss: 1.8363e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.7563e-05 - val_loss: 1.8304e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.6074e-05 - val_loss: 1.8356e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.6413e-05 - val_loss: 1.8291e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - loss: 2.6670e-05 - val_loss: 1.8385e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - loss: 2.6745e-05 - val_loss: 1.8212e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - loss: 2.6737e-05 - val_loss: 1.8578e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - loss: 2.7836e-05 - val_loss: 1.8695e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - loss: 2.6014e-05 - val_loss: 1.8441e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "# Fetch the most recent API data\n",
    "data = requests.get(\"https://blogcontent.site/projects/aqi24.php\").json()\n",
    "data = data[::-1]  # Reverse the data to maintain correct chronological order\n",
    "df_recent = pd.DataFrame(data)\n",
    "df_recent['time'] = pd.to_datetime(df_recent['time'])\n",
    "df_recent.set_index('time', inplace=True)\n",
    "\n",
    "# Load historical data\n",
    "file_path = './data/history_data.csv'  # Change this to the correct file path\n",
    "df_history = pd.read_csv(file_path)\n",
    "\n",
    "# Combine recent and historical data\n",
    "df_history['DateTime'] = pd.to_datetime(df_history['DateTime'])\n",
    "df_history.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Only use the AQI column\n",
    "df_combined = pd.concat([df_history[['AQI']], df_recent[['aqi_val']].rename(columns={'aqi_val': 'AQI'})])\n",
    "\n",
    "# Convert AQI column to numeric\n",
    "df_combined['AQI'] = pd.to_numeric(df_combined['AQI'], errors='coerce')\n",
    "\n",
    "# Apply differencing to make the series stationary\n",
    "df_combined['AQI_diff'] = df_combined['AQI'].diff().dropna()\n",
    "\n",
    "# Create a trend feature (1 for increase, 0 for decrease)\n",
    "df_combined['trend'] = np.where(df_combined['AQI_diff'] > 0, 1, 0)\n",
    "\n",
    "# Scale the differenced AQI data and trend using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_aqi = scaler.fit_transform(df_combined[['AQI_diff', 'trend']].dropna())\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(data, time_steps=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps][0])  # Predicting the AQI_diff\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set time steps as 24 (for 24-hour sequences)\n",
    "time_steps = 24\n",
    "X, y = create_sequences(scaled_aqi, time_steps)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding Bidirectional LSTM layer with 64 units\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(time_steps, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Adding another Bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Adding Dense output layer for predicting AQI_diff\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with more epochs\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Save the model as model.h5 (HDF5 format)\n",
    "model.save('model.h5')\n",
    "\n",
    "# Save the scaler for future use\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler_model.pkl')\n",
    "\n",
    "print(\"Model and Scaler saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Forecasted AQI values for the next 8 hours: [62.18741  62.42733  62.71162  63.003086 63.243217 63.395035 63.50635\n",
      " 63.574474]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import requests\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model = load_model('model.h5')\n",
    "scaler = joblib.load('scaler_model.pkl')\n",
    "\n",
    "# Input new data\n",
    "new_data = requests.get(\"https://blogcontent.site/projects/aqi24.php\").json()\n",
    "new_data = new_data[::-1]\n",
    "\n",
    "# Convert the input data to a DataFrame\n",
    "df_new = pd.DataFrame(new_data)\n",
    "df_new['time'] = pd.to_datetime(df_new['time'])\n",
    "df_new.set_index('time', inplace=True)\n",
    "\n",
    "# Convert 'aqi_val' column to numeric\n",
    "df_new['aqi_val'] = pd.to_numeric(df_new['aqi_val'])\n",
    "\n",
    "# Create difference and trend columns\n",
    "df_new['AQI_diff'] = df_new['aqi_val'].diff().dropna()\n",
    "df_new['trend'] = np.where(df_new['AQI_diff'] > 0, 1, 0)\n",
    "\n",
    "# Scale the input data using the saved scaler\n",
    "scaled_input = scaler.transform(df_new[['AQI_diff', 'trend']].dropna())\n",
    "\n",
    "# Prepare input for LSTM (handle smaller data cases)\n",
    "time_steps = 24  # Ensure this matches the training setup\n",
    "\n",
    "if len(scaled_input) < time_steps:\n",
    "    # Pad the input with zeros if there are fewer than `time_steps` data points\n",
    "    padding = np.zeros((time_steps - len(scaled_input), 2))\n",
    "    X_input = np.vstack([padding, scaled_input])\n",
    "else:\n",
    "    # Use the last `time_steps` data points for prediction\n",
    "    X_input = scaled_input[-time_steps:]\n",
    "\n",
    "# Reshape the input to fit the model input shape\n",
    "X_input = X_input.reshape(1, time_steps, 2)\n",
    "\n",
    "# Forecast the next 8 hours\n",
    "n_forecast = 8\n",
    "forecast_diff = []\n",
    "for i in range(n_forecast):\n",
    "    next_pred = model.predict(X_input)\n",
    "    forecast_diff.append(next_pred[0, 0])\n",
    "\n",
    "    # Update the input sequence for the next prediction\n",
    "    new_trend = 1 if next_pred > 0 else 0  # Update trend based on prediction\n",
    "    next_input = np.array([[next_pred[0, 0], new_trend]]).reshape(1, 1, 2)\n",
    "    X_input = np.append(X_input[:, 1:, :], next_input, axis=1)\n",
    "\n",
    "# Revert scaling\n",
    "forecast_diff = np.array(forecast_diff).reshape(-1, 1)\n",
    "forecast_diff = scaler.inverse_transform(np.hstack([forecast_diff, np.zeros_like(forecast_diff)]))[:, 0]\n",
    "\n",
    "# Revert differencing to get actual AQI forecast values\n",
    "last_known_aqi = df_new['aqi_val'].iloc[-1]\n",
    "forecast = last_known_aqi + np.cumsum(forecast_diff)\n",
    "\n",
    "# Print the forecasted AQI values for the next 8 hours\n",
    "print(\"Forecasted AQI values for the next 8 hours:\", forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input new data\n",
    "new_data = requests.get(\"https://blogcontent.site/projects/aqi24.php\").json()\n",
    "new_data = new_data[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aqi_name': 'PM10', 'aqi_val': '52.74', 'time': '2024-09-17 01:38:33.943268'}, {'aqi_name': 'PM10', 'aqi_val': '53.05', 'time': '2024-09-17 02:18:41.744855'}, {'aqi_name': 'PM10', 'aqi_val': '53.34', 'time': '2024-09-17 03:19:04.697004'}, {'aqi_name': 'PM10', 'aqi_val': '53.9', 'time': '2024-09-17 04:19:27.582172'}, {'aqi_name': 'PM10', 'aqi_val': '54.58', 'time': '2024-09-17 05:19:50.580556'}, {'aqi_name': 'PM10', 'aqi_val': '55.1', 'time': '2024-09-17 06:00:06.157689'}, {'aqi_name': 'PM10', 'aqi_val': '55.99', 'time': '2024-09-17 07:00:29.402049'}, {'aqi_name': 'PM10', 'aqi_val': '57.18', 'time': '2024-09-17 08:00:52.227380'}, {'aqi_name': 'PM10', 'aqi_val': '58.55', 'time': '2024-09-17 09:01:15.922779'}, {'aqi_name': 'PM10', 'aqi_val': '59.99', 'time': '2024-09-17 10:01:39.000063'}, {'aqi_name': 'PM10', 'aqi_val': '60.98', 'time': '2024-09-17 11:02:02.099594'}, {'aqi_name': 'PM10', 'aqi_val': '61.75', 'time': '2024-09-17 12:02:25.180874'}, {'aqi_name': 'PM10', 'aqi_val': '62.51', 'time': '2024-09-17 13:02:48.667292'}, {'aqi_name': 'PM10', 'aqi_val': '63.26', 'time': '2024-09-17 14:03:12.121067'}, {'aqi_name': 'PM10', 'aqi_val': '64.24', 'time': '2024-09-17 15:03:35.393804'}, {'aqi_name': 'PM10', 'aqi_val': '65.27', 'time': '2024-09-17 16:03:58.837359'}, {'aqi_name': 'PM10', 'aqi_val': '66.56', 'time': '2024-09-17 17:04:22.806747'}, {'aqi_name': 'PM10', 'aqi_val': '67.47', 'time': '2024-09-17 18:04:46.179405'}, {'aqi_name': 'PM10', 'aqi_val': '67.88', 'time': '2024-09-17 19:05:09.827076'}, {'aqi_name': 'PM10', 'aqi_val': '67.96', 'time': '2024-09-17 20:05:32.999857'}, {'aqi_name': 'PM10', 'aqi_val': '67.96', 'time': '2024-09-17 21:05:50.832286'}, {'aqi_name': 'PM10', 'aqi_val': '68.44', 'time': '2024-09-17 22:06:07.575900'}, {'aqi_name': 'PM10', 'aqi_val': '69.04', 'time': '2024-09-17 23:06:24.147188'}, {'aqi_name': 'PM10', 'aqi_val': '69.33', 'time': '2024-09-18 00:06:40.598625'}, {'aqi_name': 'PM10', 'aqi_val': '69.28', 'time': '2024-09-18 01:06:57.049742'}, {'aqi_name': 'PM10', 'aqi_val': '69.3', 'time': '2024-09-18 02:07:13.551865'}, {'aqi_name': 'PM10', 'aqi_val': '69.27', 'time': '2024-09-18 03:02:15.751087'}, {'aqi_name': 'PM10', 'aqi_val': '69.15', 'time': '2024-09-18 04:02:32.110121'}, {'aqi_name': 'PM10', 'aqi_val': '68.93', 'time': '2024-09-18 05:02:48.338217'}, {'aqi_name': 'PM10', 'aqi_val': '68.62', 'time': '2024-09-18 06:03:04.705437'}, {'aqi_name': 'PM10', 'aqi_val': '68.14', 'time': '2024-09-18 07:03:21.393328'}, {'aqi_name': 'PM10', 'aqi_val': '67.56', 'time': '2024-09-18 08:03:37.973183'}, {'aqi_name': 'PM10', 'aqi_val': '66.85', 'time': '2024-09-18 09:03:54.402509'}, {'aqi_name': 'PM10', 'aqi_val': '66.31', 'time': '2024-09-18 10:04:10.945180'}, {'aqi_name': 'PM10', 'aqi_val': '65.88', 'time': '2024-09-18 11:04:27.443444'}, {'aqi_name': 'PM10', 'aqi_val': '65.36', 'time': '2024-09-18 12:04:44.216787'}, {'aqi_name': 'PM10', 'aqi_val': '64.79', 'time': '2024-09-18 13:05:00.804501'}, {'aqi_name': 'PM10', 'aqi_val': '64.33', 'time': '2024-09-18 14:05:17.597235'}, {'aqi_name': 'PM10', 'aqi_val': '63.81', 'time': '2024-09-18 15:05:35.109457'}, {'aqi_name': 'PM10', 'aqi_val': '63.3', 'time': '2024-09-18 16:05:51.770424'}, {'aqi_name': 'PM10', 'aqi_val': '62.52', 'time': '2024-09-18 17:06:08.642918'}, {'aqi_name': 'PM10', 'aqi_val': '61.93', 'time': '2024-09-18 18:06:25.424127'}]\n"
     ]
    }
   ],
   "source": [
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aqi_name</th>\n",
       "      <th>aqi_val</th>\n",
       "      <th>AQI_diff</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-17 01:38:33.943268</th>\n",
       "      <td>PM10</td>\n",
       "      <td>52.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 02:18:41.744855</th>\n",
       "      <td>PM10</td>\n",
       "      <td>53.05</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 03:19:04.697004</th>\n",
       "      <td>PM10</td>\n",
       "      <td>53.34</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 04:19:27.582172</th>\n",
       "      <td>PM10</td>\n",
       "      <td>53.90</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 05:19:50.580556</th>\n",
       "      <td>PM10</td>\n",
       "      <td>54.58</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 06:00:06.157689</th>\n",
       "      <td>PM10</td>\n",
       "      <td>55.10</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 07:00:29.402049</th>\n",
       "      <td>PM10</td>\n",
       "      <td>55.99</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 08:00:52.227380</th>\n",
       "      <td>PM10</td>\n",
       "      <td>57.18</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 09:01:15.922779</th>\n",
       "      <td>PM10</td>\n",
       "      <td>58.55</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 10:01:39.000063</th>\n",
       "      <td>PM10</td>\n",
       "      <td>59.99</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 11:02:02.099594</th>\n",
       "      <td>PM10</td>\n",
       "      <td>60.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 12:02:25.180874</th>\n",
       "      <td>PM10</td>\n",
       "      <td>61.75</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 13:02:48.667292</th>\n",
       "      <td>PM10</td>\n",
       "      <td>62.51</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 14:03:12.121067</th>\n",
       "      <td>PM10</td>\n",
       "      <td>63.26</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 15:03:35.393804</th>\n",
       "      <td>PM10</td>\n",
       "      <td>64.24</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 16:03:58.837359</th>\n",
       "      <td>PM10</td>\n",
       "      <td>65.27</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 17:04:22.806747</th>\n",
       "      <td>PM10</td>\n",
       "      <td>66.56</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 18:04:46.179405</th>\n",
       "      <td>PM10</td>\n",
       "      <td>67.47</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 19:05:09.827076</th>\n",
       "      <td>PM10</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 20:05:32.999857</th>\n",
       "      <td>PM10</td>\n",
       "      <td>67.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 21:05:50.832286</th>\n",
       "      <td>PM10</td>\n",
       "      <td>67.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 22:06:07.575900</th>\n",
       "      <td>PM10</td>\n",
       "      <td>68.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17 23:06:24.147188</th>\n",
       "      <td>PM10</td>\n",
       "      <td>69.04</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 00:06:40.598625</th>\n",
       "      <td>PM10</td>\n",
       "      <td>69.33</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 01:06:57.049742</th>\n",
       "      <td>PM10</td>\n",
       "      <td>69.28</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 02:07:13.551865</th>\n",
       "      <td>PM10</td>\n",
       "      <td>69.30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 03:02:15.751087</th>\n",
       "      <td>PM10</td>\n",
       "      <td>69.27</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 04:02:32.110121</th>\n",
       "      <td>PM10</td>\n",
       "      <td>69.15</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 05:02:48.338217</th>\n",
       "      <td>PM10</td>\n",
       "      <td>68.93</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 06:03:04.705437</th>\n",
       "      <td>PM10</td>\n",
       "      <td>68.62</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 07:03:21.393328</th>\n",
       "      <td>PM10</td>\n",
       "      <td>68.14</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 08:03:37.973183</th>\n",
       "      <td>PM10</td>\n",
       "      <td>67.56</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 09:03:54.402509</th>\n",
       "      <td>PM10</td>\n",
       "      <td>66.85</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 10:04:10.945180</th>\n",
       "      <td>PM10</td>\n",
       "      <td>66.31</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 11:04:27.443444</th>\n",
       "      <td>PM10</td>\n",
       "      <td>65.88</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 12:04:44.216787</th>\n",
       "      <td>PM10</td>\n",
       "      <td>65.36</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 13:05:00.804501</th>\n",
       "      <td>PM10</td>\n",
       "      <td>64.79</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 14:05:17.597235</th>\n",
       "      <td>PM10</td>\n",
       "      <td>64.33</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 15:05:35.109457</th>\n",
       "      <td>PM10</td>\n",
       "      <td>63.81</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 16:05:51.770424</th>\n",
       "      <td>PM10</td>\n",
       "      <td>63.30</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 17:06:08.642918</th>\n",
       "      <td>PM10</td>\n",
       "      <td>62.52</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18 18:06:25.424127</th>\n",
       "      <td>PM10</td>\n",
       "      <td>61.93</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           aqi_name  aqi_val  AQI_diff  trend\n",
       "time                                                         \n",
       "2024-09-17 01:38:33.943268     PM10    52.74       NaN      0\n",
       "2024-09-17 02:18:41.744855     PM10    53.05      0.31      1\n",
       "2024-09-17 03:19:04.697004     PM10    53.34      0.29      1\n",
       "2024-09-17 04:19:27.582172     PM10    53.90      0.56      1\n",
       "2024-09-17 05:19:50.580556     PM10    54.58      0.68      1\n",
       "2024-09-17 06:00:06.157689     PM10    55.10      0.52      1\n",
       "2024-09-17 07:00:29.402049     PM10    55.99      0.89      1\n",
       "2024-09-17 08:00:52.227380     PM10    57.18      1.19      1\n",
       "2024-09-17 09:01:15.922779     PM10    58.55      1.37      1\n",
       "2024-09-17 10:01:39.000063     PM10    59.99      1.44      1\n",
       "2024-09-17 11:02:02.099594     PM10    60.98      0.99      1\n",
       "2024-09-17 12:02:25.180874     PM10    61.75      0.77      1\n",
       "2024-09-17 13:02:48.667292     PM10    62.51      0.76      1\n",
       "2024-09-17 14:03:12.121067     PM10    63.26      0.75      1\n",
       "2024-09-17 15:03:35.393804     PM10    64.24      0.98      1\n",
       "2024-09-17 16:03:58.837359     PM10    65.27      1.03      1\n",
       "2024-09-17 17:04:22.806747     PM10    66.56      1.29      1\n",
       "2024-09-17 18:04:46.179405     PM10    67.47      0.91      1\n",
       "2024-09-17 19:05:09.827076     PM10    67.88      0.41      1\n",
       "2024-09-17 20:05:32.999857     PM10    67.96      0.08      1\n",
       "2024-09-17 21:05:50.832286     PM10    67.96      0.00      0\n",
       "2024-09-17 22:06:07.575900     PM10    68.44      0.48      1\n",
       "2024-09-17 23:06:24.147188     PM10    69.04      0.60      1\n",
       "2024-09-18 00:06:40.598625     PM10    69.33      0.29      1\n",
       "2024-09-18 01:06:57.049742     PM10    69.28     -0.05      0\n",
       "2024-09-18 02:07:13.551865     PM10    69.30      0.02      1\n",
       "2024-09-18 03:02:15.751087     PM10    69.27     -0.03      0\n",
       "2024-09-18 04:02:32.110121     PM10    69.15     -0.12      0\n",
       "2024-09-18 05:02:48.338217     PM10    68.93     -0.22      0\n",
       "2024-09-18 06:03:04.705437     PM10    68.62     -0.31      0\n",
       "2024-09-18 07:03:21.393328     PM10    68.14     -0.48      0\n",
       "2024-09-18 08:03:37.973183     PM10    67.56     -0.58      0\n",
       "2024-09-18 09:03:54.402509     PM10    66.85     -0.71      0\n",
       "2024-09-18 10:04:10.945180     PM10    66.31     -0.54      0\n",
       "2024-09-18 11:04:27.443444     PM10    65.88     -0.43      0\n",
       "2024-09-18 12:04:44.216787     PM10    65.36     -0.52      0\n",
       "2024-09-18 13:05:00.804501     PM10    64.79     -0.57      0\n",
       "2024-09-18 14:05:17.597235     PM10    64.33     -0.46      0\n",
       "2024-09-18 15:05:35.109457     PM10    63.81     -0.52      0\n",
       "2024-09-18 16:05:51.770424     PM10    63.30     -0.51      0\n",
       "2024-09-18 17:06:08.642918     PM10    62.52     -0.78      0\n",
       "2024-09-18 18:06:25.424127     PM10    61.93     -0.59      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (use your local file or API)\n",
    "input_data = requests.get(\"https://blogcontent.site/projects/aqi24.php\").json()\n",
    "input_data = input_data[::-1]  # Reverse the order of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Timestamp' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([historical_data, input_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# combined_data = input_df.copy()  # Assuming input_df is the entire dataset for now\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDateTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Normalize the data\u001b[39;00m\n\u001b[1;32m     37\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:7187\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m   7185\u001b[0m         ascending \u001b[38;5;241m=\u001b[39m ascending[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 7187\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43mnargsort\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\n\u001b[1;32m   7189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   7191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/sorting.py:439\u001b[0m, in \u001b[0;36mnargsort\u001b[0;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[1;32m    437\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    438\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 439\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[\u001b[43mnon_nans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[1;32m    441\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'Timestamp' and 'str'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_data = requests.get(\"https://blogcontent.site/projects/aqi24.php\").json()\n",
    "input_data = input_data[::-1]  # Reverse the order of the data\n",
    "\n",
    "# Convert input data to DataFrame\n",
    "input_df = pd.DataFrame(input_data)\n",
    "# Explicitly convert the 'time' column to datetime format\n",
    "input_df['time'] = pd.to_datetime(input_df['time'], errors='coerce')\n",
    "\n",
    "# Convert 'aqi_val' to float\n",
    "input_df['aqi_val'] = input_df['aqi_val'].astype(float)\n",
    "\n",
    "# Filter out any rows with invalid 'time' values (e.g., NaT values)\n",
    "input_df = input_df.dropna(subset=['time'])\n",
    "\n",
    "# Continue with relevant columns\n",
    "input_df = input_df[['time', 'aqi_val']]\n",
    "input_df.columns = ['DateTime', 'PM10']\n",
    "\n",
    "\n",
    "# Load additional historical data (if available)\n",
    "historical_data = pd.read_csv('./data/history_data.csv')\n",
    "combined_data = pd.concat([historical_data, input_df], ignore_index=True)\n",
    "\n",
    "# combined_data = input_df.copy()  # Assuming input_df is the entire dataset for now\n",
    "combined_data = combined_data.sort_values(by='DateTime')\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(combined_data['PM10'].values.reshape(-1, 1))\n",
    "\n",
    "# Define a function to create a dataset with a sliding window approach\n",
    "def create_dataset(dataset, time_step=8):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - time_step):\n",
    "        X.append(dataset[i:i + time_step, 0])\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Define time step\n",
    "time_step = 8\n",
    "\n",
    "# Create training and testing datasets by removing the last 8 time steps for testing\n",
    "train_size = len(scaled_data) - 8\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size - time_step:]  # Last 8 steps are for testing\n",
    "\n",
    "# Generate training dataset\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "\n",
    "# Reshape for LSTM input [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "\n",
    "# Generate test dataset\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build LSTM Model with more units\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(time_step, 1)))  # Increased units\n",
    "model.add(LSTM(64, return_sequences=False))  # Increased units\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model with validation split\n",
    "history = model.fit(X_train, y_train, batch_size=1, epochs=100, validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "# Predict using the model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "predicted_values = scaler.inverse_transform(predictions)\n",
    "actual_values = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluation Metrics\n",
    "mae = mean_absolute_error(actual_values, predicted_values)\n",
    "mse = mean_squared_error(actual_values, predicted_values)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(actual_values, label='Actual PM10')\n",
    "plt.plot(predicted_values, label='Predicted PM10', linestyle='--')\n",
    "plt.title('PM10 Prediction vs Actual')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('PM10')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot loss during training to observe performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
